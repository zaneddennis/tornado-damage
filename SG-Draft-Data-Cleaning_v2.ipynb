{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package import cell\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH',\n",
       "       'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE', 'STATE_FIPS',\n",
       "       'YEAR', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME',\n",
       "       'WFO', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'END_DATE_TIME',\n",
       "       'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT',\n",
       "       'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'SOURCE',\n",
       "       'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'CATEGORY', 'TOR_F_SCALE',\n",
       "       'TOR_LENGTH', 'TOR_WIDTH', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE',\n",
       "       'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'BEGIN_RANGE',\n",
       "       'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH',\n",
       "       'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON',\n",
       "       'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'DATA_SOURCE',\n",
       "       'State FIPS Code', 'Name', 'Median Household Income', 'Id', 'Id2',\n",
       "       'Geography', 'Target Geo Id', 'Target Geo Id2', 'Geographic area',\n",
       "       'Geographic area.1', 'Population', 'Housing units',\n",
       "       'Area in square miles - Total area',\n",
       "       'Area in square miles - Water area', 'Area in square miles - Land area',\n",
       "       'Density per square mile of land area - Population',\n",
       "       'Density per square mile of land area - Housing units'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load merged tornadoes+income+density data and view all columns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "data = pd.read_csv('Merged-Tornadoes.csv',index_col = 0)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single unique 'STATE: COUNTY' column and trim redundant geographic columns\n",
    "\n",
    "data['STATE: COUNTY'] = data['STATE'].map(str)+': '+data['CZ_NAME']\n",
    "\n",
    "data = data.drop(columns=['STATE', 'STATE_FIPS', 'CZ_FIPS', 'CZ_NAME',\n",
    "                        'State FIPS Code', 'Name', \n",
    "                        'Geography', 'Geographic area', 'Geographic area.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['INJURIES_DIRECT'] = pd.to_numeric(data['INJURIES_DIRECT'])\n",
    "data['INJURIES_INDIRECT'] = pd.to_numeric(data['INJURIES_INDIRECT'])\n",
    "data['DEATHS_DIRECT'] = pd.to_numeric(data['DEATHS_DIRECT'])\n",
    "data['DEATHS_INDIRECT'] = pd.to_numeric(data['DEATHS_INDIRECT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove columns that are not useful (e.g., 'EVENT TYPE' is now only Tornado)\n",
    "\n",
    "data = data.drop(columns=['EVENT_TYPE', 'FLOOD_CAUSE', 'CATEGORY', 'Id', 'Id2',\n",
    "                          'END_YEARMONTH', 'BEGIN_YEARMONTH', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE',\n",
    "                          'END_DATE_TIME', 'CZ_TYPE', 'WFO','DATA_SOURCE',\n",
    "                          'Target Geo Id', 'Target Geo Id2', 'Population', 'Housing units',\n",
    "                          'Area in square miles - Water area', 'Area in square miles - Land area',\n",
    "                          'Area in square miles - Total area', 'END_DAY', 'END_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"CASUALTIES\"] = data[\"INJURIES_INDIRECT\"] + data[\"INJURIES_DIRECT\"] + data[\"DEATHS_DIRECT\"] + data[\"DEATHS_INDIRECT\"]\n",
    "data = data.drop(columns=[\"INJURIES_INDIRECT\", \"INJURIES_DIRECT\", \"DEATHS_INDIRECT\", \"DEATHS_DIRECT\"])\n",
    "data = data.drop(columns=[\"TOR_OTHER_WFO\", \"TOR_OTHER_CZ_STATE\", \"TOR_OTHER_CZ_FIPS\", \"TOR_OTHER_CZ_NAME\", 'MAGNITUDE', 'MAGNITUDE_TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of data for baseline model\n",
    "data = data.drop(columns=['MONTH_NAME','SOURCE','EPISODE_NARRATIVE','EVENT_NARRATIVE','BEGIN_LOCATION','END_LOCATION','BEGIN_AZIMUTH','END_AZIMUTH','STATE: COUNTY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting F_Scale to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['TOR_F_SCALE'] != 'EFU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TOR_F_SCALE'] = data['TOR_F_SCALE'].map(lambda x: int(x.lstrip('EF')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to sin and cos time\n",
    "\n",
    "Here we are creating sin_time and cos_time to correctly convey the cyclical nature of time. 0001 and 2359 are 2 minutes apart and we want the model to understand that. Converting to sin_time or cos_time alone doesn't solve the problem as the sin and cos function are symmetrical and 2 different times can be represented by the same sin or cos value. Hence, we use both the sin and cos functions to create a unique cyclical value for each event.\n",
    "<br> <br>\n",
    "Reference: https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359\n"
     ]
    }
   ],
   "source": [
    "# Check if 24 hour clock\n",
    "print(data['BEGIN_TIME'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_in_a_day = 2400\n",
    "data['sin_time'] = np.sin(2*np.pi*data.BEGIN_TIME/hours_in_a_day)\n",
    "data['cos_time'] = np.cos(2*np.pi*data.BEGIN_TIME/hours_in_a_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This visual illustrates how time is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data.sample(50).plot.scatter('sin_time','cos_time').set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop EPISODE_ID, DAMAGE_CROPS, TOR_F_SCALE\n",
    "# TOR_F_SCALE can be used as a proxy for intensity and wind speed. However, it is not included in the baseline\n",
    "# model as it is assigned after the tornado occurs and we want to avoid potential data leakage.\n",
    "data = data.drop(columns = ['EPISODE_ID','DAMAGE_CROPS','TOR_F_SCALE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('EVENT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BEGIN_DAY                                                  0\n",
       "BEGIN_TIME                                                 0\n",
       "YEAR                                                       0\n",
       "DAMAGE_PROPERTY                                         1610\n",
       "TOR_LENGTH                                                 0\n",
       "TOR_WIDTH                                                  0\n",
       "BEGIN_RANGE                                                0\n",
       "END_RANGE                                                  0\n",
       "BEGIN_LAT                                                  0\n",
       "BEGIN_LON                                                  0\n",
       "END_LAT                                                    0\n",
       "END_LON                                                    0\n",
       "Median Household Income                                  543\n",
       "Density per square mile of land area - Population        576\n",
       "Density per square mile of land area - Housing units     576\n",
       "CASUALTIES                                                 0\n",
       "sin_time                                                   0\n",
       "cos_time                                                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for NaN values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning target variable DAMAGE_PROPERTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['DAMAGE_PROPERTY'] = data['DAMAGE_PROPERTY'].str.replace('.00K','000')\n",
    "data['DAMAGE_PROPERTY'] = data['DAMAGE_PROPERTY'].str.replace('.00M','000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_billions (value):\n",
    "    value = value[:-1]\n",
    "    value = float(value) * 1000000000\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_millions (value):\n",
    "    value = value[:-1]\n",
    "    value = float(value) * 1000000   \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_thousands (value):\n",
    "    value = value[:-1]\n",
    "    value = float(value) * 1000 \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data.loc[data['DAMAGE_PROPERTY'].str.endswith('B') == True,'DAMAGE_PROPERTY'] = data.loc[data['DAMAGE_PROPERTY'].str.endswith('B') == True,'DAMAGE_PROPERTY'].apply(convert_to_billions)\n",
    "data.loc[data['DAMAGE_PROPERTY'].str.endswith('M') == True,'DAMAGE_PROPERTY'] = data.loc[data['DAMAGE_PROPERTY'].str.endswith('M') == True,'DAMAGE_PROPERTY'].apply(convert_to_millions)\n",
    "data.loc[data['DAMAGE_PROPERTY'].str.endswith('K') == True,'DAMAGE_PROPERTY'] = data.loc[data['DAMAGE_PROPERTY'].str.endswith('K') == True,'DAMAGE_PROPERTY'].apply(convert_to_thousands)\n",
    "\n",
    "print(data['DAMAGE_PROPERTY'].str.endswith('B').sum())\n",
    "print(data['DAMAGE_PROPERTY'].str.endswith('K').sum())\n",
    "print(data['DAMAGE_PROPERTY'].str.endswith('M').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean of DAMAGE_PROPERTY for filling NaN values\n",
    "# Step 1: Converting DAMAGE_PROPERTY to float \n",
    "data['DAMAGE_PROPERTY'] = data['DAMAGE_PROPERTY'].astype(float)\n",
    "# Step 2: Skip NaN values when calculating mean\n",
    "mean_DAMAGE_PROPERTY = data['DAMAGE_PROPERTY'].mean(skipna = True)\n",
    "# Step 3: Filling NaN values\n",
    "data['DAMAGE_PROPERTY'] = data['DAMAGE_PROPERTY'].fillna(mean_DAMAGE_PROPERTY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating average of begin range and end range, which is a proxy for distance from population center\n",
    "data['AVG_RANGE'] = data.loc[: , ['BEGIN_RANGE','END_RANGE']].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean of Median_Household_Income for filling NaN values\n",
    "# Step 1: Remove commas from Median_Household_Income, converting it to float \n",
    "data['Median Household Income'] = data['Median Household Income'].str.replace(\",\", \"\").astype(float)\n",
    "# Step 2: Skip NaN values when calculating mean\n",
    "mean_Median_Household_Income = data['Median Household Income'].mean(skipna = True)\n",
    "# Step 3: Filling NaN values\n",
    "data['Median Household Income'] = data['Median Household Income'].fillna(mean_Median_Household_Income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Renamed Density per square mile of land area - Population and Density per square mile of land area - Housing units\n",
    "data = data.rename(columns={'Density per square mile of land area - Population': 'Pop_Density','Density per square mile of land area - Housing units': 'Housing_Units_Density'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean of Pop_Density for filling NaN values\n",
    "# Step 1: Converting Pop_Density to float \n",
    "data['Pop_Density'] = data['Pop_Density'].astype(float)\n",
    "# Step 2: Skip NaN values when calculating mean\n",
    "mean_Pop_Density = data['Pop_Density'].mean(skipna = True)\n",
    "# Step 3: Filling NaN values\n",
    "data['Pop_Density'] = data['Pop_Density'].fillna(mean_Pop_Density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean of Housing_Units_Density for filling NaN values\n",
    "# Step 1: Converting Pop_Density to float \n",
    "data['Housing_Units_Density'] = data['Housing_Units_Density'].astype(float)\n",
    "# Step 2: Skip NaN values when calculating mean\n",
    "mean_Housing_Units_Density = data['Housing_Units_Density'].mean(skipna = True)\n",
    "# Step 3: Filling NaN values\n",
    "data['Housing_Units_Density'] = data['Housing_Units_Density'].fillna(mean_Housing_Units_Density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BEGIN_DAY                  0\n",
       "BEGIN_TIME                 0\n",
       "YEAR                       0\n",
       "DAMAGE_PROPERTY            0\n",
       "TOR_LENGTH                 0\n",
       "TOR_WIDTH                  0\n",
       "BEGIN_RANGE                0\n",
       "END_RANGE                  0\n",
       "BEGIN_LAT                  0\n",
       "BEGIN_LON                  0\n",
       "END_LAT                    0\n",
       "END_LON                    0\n",
       "Median Household Income    0\n",
       "Pop_Density                0\n",
       "Housing_Units_Density      0\n",
       "CASUALTIES                 0\n",
       "sin_time                   0\n",
       "cos_time                   0\n",
       "AVG_RANGE                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for NaN values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Changes made__:\n",
    "- 'BEGIN_DAY' - ignore for now\n",
    "- 'BEGIN_TIME' - check if 24 hours - and normalize with cosine\n",
    "- 'EPISODE_ID' - drop for now\n",
    "- 'EVENT_ID' - make index\n",
    "- 'YEAR' - not scaling for decision tree, can scale for other models\n",
    "- 'DAMAGE_CROPS' - drop for now\n",
    "- 'TOR_F_SCALE' - Note: Using as a proxy for intensity and wind speed, drop column for now\n",
    "-  New column averaging 'BEGIN_RANGE' and 'END_RANGE'\n",
    "- 'Median Household Income' - use average for NaNs\n",
    "- 'Density per square mile of land area - Population' - use average for NaNs\n",
    "- 'Density per square mile of land area - Housing units'- use average for NaNs\n",
    "- 'DAMAGE_PROPERTY' - target variable - more relevant to public safety - multiplied according to units\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Cleaned-Tornadoes_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(data[data.columns[data.columns != 'CASUALTIES']], data[data.columns[data.columns == 'CASUALTIES']], test_size=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
